{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znlW5ecuQutZ"
      },
      "source": [
        "**Preference Fine-Tuning of TinyLlama**\n",
        "\n",
        "**Assignment 05 - Part 1: SFT Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w4eSMX8wuywR",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes accelerate\n",
        "!pip install trl\n",
        "!pip install sacrebleu\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT49tMtsqKqY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "import time\n",
        "import torch\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType,\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer\n",
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from evaluate import load\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OunaJx7bqPlL"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na0sL1HNYLN0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv9SNvP6qPgI"
      },
      "outputs": [],
      "source": [
        "def clear_hf_cache():\n",
        "    \"\"\"Clear Hugging Face cache to avoid conflicts\"\"\"\n",
        "    cache_dirs = [\n",
        "        os.path.expanduser(\"~/.cache/huggingface/datasets\"),\n",
        "        \"/root/.cache/huggingface/datasets\",\n",
        "        \"/kaggle/working/.cache/huggingface/datasets\",\n",
        "    ]\n",
        "\n",
        "    for cache_dir in cache_dirs:\n",
        "        if os.path.exists(cache_dir):\n",
        "            try:\n",
        "                shutil.rmtree(cache_dir)\n",
        "                print(f\"‚úì Cleared cache: {cache_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† Could not clear {cache_dir}: {e}\")\n",
        "\n",
        "def load_openassistant_dataset(max_samples=None):\n",
        "    \"\"\"Load OpenAssistant dataset from parquet files\"\"\"\n",
        "    print(\"üîÑ Loading OpenAssistant dataset...\")\n",
        "\n",
        "    clear_hf_cache()\n",
        "\n",
        "    try:\n",
        "        # URLs for the parquet files\n",
        "        train_url = \"https://huggingface.co/datasets/OpenAssistant/oasst1/resolve/main/data/train-00000-of-00001-b42a775f407cee45.parquet\"\n",
        "        val_url = \"https://huggingface.co/datasets/OpenAssistant/oasst1/resolve/main/data/validation-00000-of-00001-134b8fd0c89408b6.parquet\"\n",
        "\n",
        "        print(\"Downloading training data...\")\n",
        "        train_response = requests.get(train_url)\n",
        "        train_response.raise_for_status()\n",
        "        train_df = pd.read_parquet(BytesIO(train_response.content))\n",
        "\n",
        "        print(\"Downloading validation data...\")\n",
        "        val_response = requests.get(val_url)\n",
        "        val_response.raise_for_status()\n",
        "        val_df = pd.read_parquet(BytesIO(val_response.content))\n",
        "\n",
        "        print(f\"‚úÖ Raw training data: {len(train_df)} messages\")\n",
        "        print(f\"‚úÖ Raw validation data: {len(val_df)} messages\")\n",
        "\n",
        "        # Process the datasets to create conversation pairs\n",
        "        train_conversations = process_oasst_data(train_df, max_samples)\n",
        "        val_conversations = process_oasst_data(val_df, max_samples//10 if max_samples else None)\n",
        "\n",
        "        # Convert to HuggingFace datasets\n",
        "        train_dataset = Dataset.from_pandas(pd.DataFrame(train_conversations))\n",
        "        val_dataset = Dataset.from_pandas(pd.DataFrame(val_conversations))\n",
        "\n",
        "        dataset = DatasetDict({\n",
        "            'train': train_dataset,\n",
        "            'validation': val_dataset\n",
        "        })\n",
        "\n",
        "        print(f\"‚úÖ Processed training conversations: {len(train_conversations)}\")\n",
        "        print(f\"‚úÖ Processed validation conversations: {len(val_conversations)}\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load dataset: {e}\")\n",
        "        raise Exception(\"Could not load dataset. Check your internet connection and try again.\")\n",
        "\n",
        "def process_oasst_data(df, max_samples=None):\n",
        "    \"\"\"\n",
        "    Process OpenAssistant data to create instruction-response pairs\n",
        "\n",
        "    The dataset has a tree structure where:\n",
        "    - Each message has a parent_id (None for root messages)\n",
        "    - Roles alternate between 'prompter' and 'assistant'\n",
        "    - We want to create pairs of (prompter message, assistant response)\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Processing OpenAssistant conversation trees...\")\n",
        "\n",
        "    # Filter for English messages only and approved messages\n",
        "    df_filtered = df[\n",
        "        (df['lang'] == 'en') &\n",
        "        (df['deleted'] == False) &\n",
        "        (df['review_result'] == True)\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"Filtered to {len(df_filtered)} quality English messages\")\n",
        "\n",
        "    # Create a mapping from message_id to message data\n",
        "    messages = {}\n",
        "    for _, row in df_filtered.iterrows():\n",
        "        messages[row['message_id']] = {\n",
        "            'text': row['text'],\n",
        "            'role': row['role'],\n",
        "            'parent_id': row['parent_id'],\n",
        "            'message_id': row['message_id']\n",
        "        }\n",
        "\n",
        "    conversations = []\n",
        "\n",
        "    # Find all prompter messages that have assistant replies\n",
        "    for msg_id, msg_data in messages.items():\n",
        "        if msg_data['role'] == 'prompter':\n",
        "            # Find assistant responses to this prompter message\n",
        "            for potential_child_id, potential_child in messages.items():\n",
        "                if (potential_child['parent_id'] == msg_id and\n",
        "                    potential_child['role'] == 'assistant'):\n",
        "\n",
        "                    # Create conversation pair\n",
        "                    conversation = {\n",
        "                        'instruction': msg_data['text'].strip(),\n",
        "                        'response': potential_child['text'].strip(),\n",
        "                        'conversation_id': f\"{msg_id}_{potential_child_id}\"\n",
        "                    }\n",
        "                    conversations.append(conversation)\n",
        "\n",
        "    print(f\"Created {len(conversations)} instruction-response pairs\")\n",
        "\n",
        "    # Limit samples if specified\n",
        "    if max_samples and max_samples < len(conversations):\n",
        "        conversations = conversations[:max_samples]\n",
        "        print(f\"üî™ Limited to {max_samples} conversation pairs\")\n",
        "\n",
        "    # Filter out very short or very long conversations\n",
        "    filtered_conversations = []\n",
        "    for conv in conversations:\n",
        "        if (len(conv['instruction']) > 10 and len(conv['response']) > 10 and\n",
        "            len(conv['instruction']) < 2000 and len(conv['response']) < 2000):\n",
        "            filtered_conversations.append(conv)\n",
        "\n",
        "    print(f\"After length filtering: {len(filtered_conversations)} conversations\")\n",
        "\n",
        "    return filtered_conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGdzRP5aqJb1"
      },
      "outputs": [],
      "source": [
        "SUBSET_SIZE = 5000\n",
        "dataset = load_openassistant_dataset(max_samples=SUBSET_SIZE)\n",
        "\n",
        "print(\"\\nüìä Dataset Information:\")\n",
        "print(f\"Dataset keys: {list(dataset.keys())}\")\n",
        "print(f\"Training samples: {len(dataset['train'])}\")\n",
        "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kiZaELXrAS6"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(f\"Loading model and tokenizer: {MODEL_NAME}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKZO7QAStxEB"
      },
      "outputs": [],
      "source": [
        "def format_instruction(sample):\n",
        "    \"\"\"Format the sample into instruction-response format for OpenAssistant\"\"\"\n",
        "    return f\"\"\"<|system|>\n",
        "You are a helpful AI assistant. Provide helpful, accurate, and detailed responses to user questions.\n",
        "<|user|>\n",
        "{sample['instruction']}\n",
        "<|assistant|>\n",
        "{sample['response']}\"\"\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Preprocess the dataset samples for OpenAssistant format\"\"\"\n",
        "    texts = []\n",
        "    for i in range(len(examples['instruction'])):\n",
        "        sample = {\n",
        "            'instruction': examples['instruction'][i],\n",
        "            'response': examples['response'][i]\n",
        "        }\n",
        "        texts.append(format_instruction(sample))\n",
        "\n",
        "    return {\"text\": texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E9EXXph86EL"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(preprocess_function, batched=True,\n",
        "                                remove_columns=train_dataset.column_names)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True,\n",
        "                            remove_columns=val_dataset.column_names)\n",
        "\n",
        "\n",
        "print(\"Sample formatted text:\")\n",
        "print(train_dataset[0]['text'][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weWVIonGOsHt"
      },
      "outputs": [],
      "source": [
        "lora_configs = {\n",
        "    # \"balanced_trial_1\": {\n",
        "    #     \"r\": 8,                    # Increased from 4 to 8\n",
        "    #     \"lora_alpha\": 32,          # Scaled proportionally (4x rank)\n",
        "    #     \"target_modules\": [\"q_proj\", \"v_proj\"],  # Core attention modules\n",
        "    #     \"lora_dropout\": 0.1,\n",
        "    # },\n",
        "    # \"balanced_trial_2\": {\n",
        "    #     \"r\": 12,                   # Increased from 6 to 12\n",
        "    #     \"lora_alpha\": 48,          # Scaled proportionally (4x rank)\n",
        "    #     \"target_modules\": [\"q_proj\", \"v_proj\"],  # Keep to 2 modules for efficiency\n",
        "    #     \"lora_dropout\": 0.1,\n",
        "    # },\n",
        "    # \"balanced_trial_3\": {\n",
        "    #     \"r\": 6,                    # Increased from 2 to 6 (3x)\n",
        "    #     \"lora_alpha\": 24,          # Scaled proportionally (4x rank)\n",
        "    #     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # All attention\n",
        "    #     \"lora_dropout\": 0.1,\n",
        "    # },\n",
        "    \"balanced_trial_4\": {\n",
        "        \"r\": 16,                   # Increased from 12 to 16\n",
        "        \"lora_alpha\": 32,          # Conservative scaling (2x rank)\n",
        "        \"target_modules\": [\"q_proj\", \"v_proj\"],  # Only key modules\n",
        "        \"lora_dropout\": 0.05,      # Keep lower dropout for high rank\n",
        "    },\n",
        "    # \"balanced_trial_5\": {\n",
        "    #     \"r\": 10,                   # Increased from 8 to 10\n",
        "    #     \"lora_alpha\": 32,          # Moderate scaling\n",
        "    #     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\"],  # 3 modules\n",
        "    #     \"lora_dropout\": 0.1,\n",
        "    # },\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TixIihEO9sL"
      },
      "outputs": [],
      "source": [
        "training_configs = {\n",
        "    # \"balanced_trial_1\": {\n",
        "    #     \"learning_rate\": 2.5e-4,   # Slightly reduced for higher rank\n",
        "    #     \"per_device_train_batch_size\": 4,  # Reduced for memory efficiency\n",
        "    #     \"num_train_epochs\": 4,      # Increased from 3\n",
        "    #     \"gradient_accumulation_steps\": 3,   # Increased to maintain effective batch size\n",
        "    # },\n",
        "    # \"balanced_trial_2\": {\n",
        "    #     \"learning_rate\": 2e-4,      # Lower for highest rank trial\n",
        "    #     \"per_device_train_batch_size\": 4,   # Consistent with trial 1\n",
        "    #     \"num_train_epochs\": 4,      # Increased from 3\n",
        "    #     \"gradient_accumulation_steps\": 3,   # Maintain effective batch size\n",
        "    # },\n",
        "    # \"balanced_trial_3\": {\n",
        "    #     \"learning_rate\": 2.5e-4,    # Moderate rate for broad coverage\n",
        "    #     \"per_device_train_batch_size\": 3,   # Lower due to 4 modules\n",
        "    #     \"num_train_epochs\": 5,      # Longer training for broad coverage\n",
        "    #     \"gradient_accumulation_steps\": 4,   # Higher accumulation to compensate\n",
        "    # },\n",
        "    \"balanced_trial_4\": {\n",
        "        \"learning_rate\": 1.8e-4,    # Most conservative for highest rank\n",
        "        \"per_device_train_batch_size\": 4,   # Maintain reasonable batch size\n",
        "        \"num_train_epochs\": 5,      # Longer training for complex model\n",
        "        \"gradient_accumulation_steps\": 3,   # Balanced accumulation\n",
        "    },\n",
        "    # \"balanced_trial_5\": {\n",
        "    #     \"learning_rate\": 2.2e-4,    # Moderate rate\n",
        "    #     \"per_device_train_batch_size\": 3,   # Lower for 3 modules\n",
        "    #     \"num_train_epochs\": 6,      # Longest training for balanced approach\n",
        "    #     \"gradient_accumulation_steps\": 4,   # Higher accumulation\n",
        "    # },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aKITdUEK6qU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def save_training_metrics(trainer, trial_name):\n",
        "    \"\"\"Save detailed training metrics\"\"\"\n",
        "    training_logs = {\n",
        "        \"train_loss_history\": trainer.state.log_history,\n",
        "        \"eval_loss_history\": [log for log in trainer.state.log_history if 'eval_loss' in log],\n",
        "        \"learning_rate_schedule\": [log.get('learning_rate', 0) for log in trainer.state.log_history],\n",
        "        \"global_steps\": trainer.state.global_step,\n",
        "        \"total_epochs\": trainer.state.epoch,\n",
        "    }\n",
        "\n",
        "    with open(f\"training_metrics_{trial_name}.json\", \"w\") as f:\n",
        "        json.dump(training_logs, f, indent=2)\n",
        "\n",
        "def save_comparison_plots(results):\n",
        "    \"\"\"Create and save comparison visualizations\"\"\"\n",
        "\n",
        "    model_names = list(evaluation_results.keys())\n",
        "    bleu_scores = [evaluation_results[name]['bleu_score'] for name in model_names]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(model_names, bleu_scores)\n",
        "    plt.title('BLEU Score Comparison Across Models')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('BLEU Score')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    best_idx = bleu_scores.index(max(bleu_scores))\n",
        "    bars[best_idx].set_color('gold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('bleu_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    training_times = [result['training_time'] for result in results]\n",
        "    trial_names = [result['trial_name'] for result in results]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(trial_names, training_times)\n",
        "    plt.title('Training Time Comparison')\n",
        "    plt.xlabel('Trial')\n",
        "    plt.ylabel('Training Time (seconds)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_time_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def save_hyperparameter_analysis(results):\n",
        "    \"\"\"Analyze relationship between hyperparameters and performance\"\"\"\n",
        "    analysis_data = []\n",
        "    print('here')\n",
        "\n",
        "    for trial_name in lora_configs.keys():\n",
        "        if trial_name in evaluation_results:\n",
        "            print('here2')\n",
        "            print(\"Type of results:\", type(results))\n",
        "            print(\"Keys of results:\", results.keys() if isinstance(results, dict) else \"Not a dict\")\n",
        "            analysis_data.append({\n",
        "                'trial_name': trial_name,\n",
        "                'bleu_score': evaluation_results[trial_name]['bleu_score'],\n",
        "                'lora_r': lora_configs[trial_name]['r'],\n",
        "                'lora_alpha': lora_configs[trial_name]['lora_alpha'],\n",
        "                'learning_rate': training_configs[trial_name]['learning_rate'],\n",
        "                'batch_size': training_configs[trial_name]['per_device_train_batch_size'],\n",
        "                'epochs': training_configs[trial_name]['num_train_epochs'],\n",
        "                'num_target_modules': len(lora_configs[trial_name]['target_modules']),\n",
        "                'training_time': next((r['training_time'] for r in results if r['trial_name'] == trial_name), \"0\")\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(analysis_data)\n",
        "    df.to_csv('hyperparameter_analysis.csv', index=False)\n",
        "\n",
        "    numeric_cols = ['bleu_score', 'lora_r', 'lora_alpha', 'learning_rate',\n",
        "                   'batch_size', 'epochs', 'num_target_modules', 'training_time']\n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Hyperparameter Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('hyperparameter_correlation.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def save_response_analysis():\n",
        "    \"\"\"Analyze response quality in detail\"\"\"\n",
        "    response_analysis = {}\n",
        "\n",
        "    for i, prompt in enumerate(evaluation_prompts):\n",
        "        prompt_analysis = {\n",
        "            'prompt': prompt,\n",
        "            'reference_answer': reference_answers[i],\n",
        "            'responses': {},\n",
        "            'response_lengths': {},\n",
        "            'contains_numbers': {}\n",
        "        }\n",
        "\n",
        "        for model_name, results in evaluation_results.items():\n",
        "            response = results['predictions'][i]\n",
        "            prompt_analysis['responses'][model_name] = response\n",
        "            prompt_analysis['response_lengths'][model_name] = len(response.split())\n",
        "            prompt_analysis['contains_numbers'][model_name] = bool(re.search(r'\\d+', response))\n",
        "\n",
        "        response_analysis[f'prompt_{i+1}'] = prompt_analysis\n",
        "\n",
        "    with open('detailed_response_analysis.json', 'w') as f:\n",
        "        json.dump(response_analysis, f, indent=2)\n",
        "\n",
        "def save_experiment_config():\n",
        "    \"\"\"Save complete experiment configuration for reproducibility\"\"\"\n",
        "    config = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'model_name': MODEL_NAME,\n",
        "        'dataset_info': {\n",
        "            'name': 'microsoft/orca-math-word-problems-200k',\n",
        "            'subset_size': SUBSET_SIZE,\n",
        "            'train_size': len(train_dataset),\n",
        "            'val_size': len(val_dataset)\n",
        "        },\n",
        "        'lora_configs': lora_configs,\n",
        "        'training_configs': training_configs,\n",
        "        'evaluation_prompts': evaluation_prompts,\n",
        "        'reference_answers': reference_answers,\n",
        "        'quantization_config': {\n",
        "            'load_in_4bit': True,\n",
        "            'bnb_4bit_quant_type': 'nf4',\n",
        "            'bnb_4bit_compute_dtype': 'float16',\n",
        "            'bnb_4bit_use_double_quant': True\n",
        "        },\n",
        "        'device_info': {\n",
        "            'device': str(device),\n",
        "            'cuda_available': torch.cuda.is_available(),\n",
        "            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open('experiment_config.json', 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "def save_model_analysis():\n",
        "    \"\"\"Save analysis of model parameters and sizes\"\"\"\n",
        "    model_analysis = {}\n",
        "\n",
        "    for trial_name in lora_configs.keys():\n",
        "        model_path = f\"./models/{trial_name}\"\n",
        "        if os.path.exists(model_path):\n",
        "            # Calculate approximate model size\n",
        "            total_size = 0\n",
        "            for root, dirs, files in os.walk(model_path):\n",
        "                for file in files:\n",
        "                    total_size += os.path.getsize(os.path.join(root, file))\n",
        "\n",
        "            model_analysis[trial_name] = {\n",
        "                'model_size_mb': total_size / (1024 * 1024),\n",
        "                'lora_parameters': lora_configs[trial_name]['r'] * 2 * len(lora_configs[trial_name]['target_modules']),\n",
        "                'target_modules': lora_configs[trial_name]['target_modules'],\n",
        "                'efficiency_score': evaluation_results.get(trial_name, {}).get('bleu_score', 0) / (total_size / (1024 * 1024))\n",
        "            }\n",
        "\n",
        "    with open('model_analysis.json', 'w') as f:\n",
        "        json.dump(model_analysis, f, indent=2)\n",
        "\n",
        "def generate_summary_report():\n",
        "    \"\"\"Generate a comprehensive markdown report\"\"\"\n",
        "    report = f\"\"\"# Supervised Fine-Tuning Experiment Report\n",
        "\n",
        "## Experiment Overview\n",
        "- **Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- **Base Model**: {MODEL_NAME}\n",
        "- **Dataset**: Microsoft Orca Math Word Problems (subset of {SUBSET_SIZE} samples)\n",
        "- **Evaluation Metric**: BLEU Score\n",
        "\n",
        "## Results Summary\n",
        "\n",
        "### Best Performing Model\n",
        "- **Model**: {best_model_name}\n",
        "- **BLEU Score**: {best_model_results['bleu_score']:.4f}\n",
        "- **Improvement over Base**: {best_model_results['bleu_score'] - evaluation_results['base_model']['bleu_score']:+.4f}\n",
        "\n",
        "### All Models Performance\n",
        "| Model | BLEU Score | Improvement |\n",
        "|-------|------------|-------------|\n",
        "\"\"\"\n",
        "\n",
        "    for model_name, results in sorted_models:\n",
        "        improvement = results['bleu_score'] - evaluation_results['base_model']['bleu_score']\n",
        "        report += f\"| {model_name} | {results['bleu_score']:.4f} | {improvement:+.4f} |\\n\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "## Hyperparameter Analysis\n",
        "\n",
        "### Best Configuration\n",
        "\"\"\"\n",
        "    if best_model_name != 'base_model':\n",
        "        report += f\"\"\"\n",
        "**LoRA Configuration:**\n",
        "- Rank (r): {best_model_results['lora_config']['r']}\n",
        "- Alpha: {best_model_results['lora_config']['lora_alpha']}\n",
        "- Target Modules: {best_model_results['lora_config']['target_modules']}\n",
        "- Dropout: {best_model_results['lora_config']['lora_dropout']}\n",
        "\n",
        "**Training Configuration:**\n",
        "- Learning Rate: {best_model_results['training_config']['learning_rate']}\n",
        "- Batch Size: {best_model_results['training_config']['per_device_train_batch_size']}\n",
        "- Epochs: {best_model_results['training_config']['num_train_epochs']}\n",
        "- Gradient Accumulation Steps: {best_model_results['training_config']['gradient_accumulation_steps']}\n",
        "\"\"\"\n",
        "\n",
        "    report += \"\\n## Files Generated\\n\"\n",
        "    generated_files = [\n",
        "        \"training_results_*.json - Individual trial results\",\n",
        "        \"all_training_results.json - Combined training results\",\n",
        "        \"bleu_evaluation_results.json - BLEU evaluation results\",\n",
        "        \"best_sft_model_info.json - Best model info for DPO\",\n",
        "        \"experiment_config.json - Complete experiment configuration\",\n",
        "        \"hyperparameter_analysis.csv - Hyperparameter analysis data\",\n",
        "        \"detailed_response_analysis.json - Detailed response analysis\",\n",
        "        \"model_analysis.json - Model size and parameter analysis\",\n",
        "        \"*.png - Visualization plots\",\n",
        "        \"./models/*/ - Trained model checkpoints\"\n",
        "    ]\n",
        "\n",
        "    for file_desc in generated_files:\n",
        "        report += f\"- {file_desc}\\n\"\n",
        "\n",
        "    with open('experiment_report.md', 'w') as f:\n",
        "        f.write(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJQc6dFat9cG"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "def train_model(trial_name, lora_config, training_config):\n",
        "    \"\"\"Train model with given configurations\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {trial_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=lora_config[\"r\"],\n",
        "        lora_alpha=lora_config[\"lora_alpha\"],\n",
        "        target_modules=lora_config[\"target_modules\"],\n",
        "        lora_dropout=lora_config[\"lora_dropout\"],\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    model_copy = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model_copy = prepare_model_for_kbit_training(model_copy)\n",
        "    model_copy = get_peft_model(model_copy, peft_config)\n",
        "\n",
        "    model_copy.print_trainable_parameters()\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{trial_name}\",\n",
        "        learning_rate=training_config[\"learning_rate\"],\n",
        "        per_device_train_batch_size=training_config[\"per_device_train_batch_size\"],\n",
        "        per_device_eval_batch_size=5,\n",
        "        num_train_epochs=training_config[\"num_train_epochs\"],\n",
        "        gradient_accumulation_steps=training_config[\"gradient_accumulation_steps\"],\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        logging_steps=100,\n",
        "        fp16=True,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model_copy,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        peft_config=peft_config,\n",
        "        formatting_func=lambda x: x[\"text\"],\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(f\"./models/{trial_name}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    save_training_metrics(trainer, trial_name)\n",
        "\n",
        "\n",
        "    print(f\"{trial_name} completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    del model_copy\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        \"trial_name\": trial_name,\n",
        "        \"training_time\": training_time,\n",
        "        \"lora_config\": lora_config,\n",
        "        \"training_config\": training_config\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33D8zQAFt_6h"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for trial_name in lora_configs.keys():\n",
        "    try:\n",
        "        result = train_model(\n",
        "            trial_name=trial_name,\n",
        "            lora_config=lora_configs[trial_name],\n",
        "            training_config=training_configs[trial_name]\n",
        "        )\n",
        "        results.append(result)\n",
        "\n",
        "        with open(f\"training_results_{trial_name}.json\", \"w\") as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in {trial_name}: {str(e)}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-LNxz78uD57"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL TRIALS COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with open(\"all_training_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqBA3jqCuPY9"
      },
      "outputs": [],
      "source": [
        "evaluation_prompts = [\n",
        "    \"What are the main differences between Python and JavaScript programming languages?\",\n",
        "    \"How can I improve my time management skills while working from home?\",\n",
        "    \"Explain the concept of climate change and its main causes.\",\n",
        "    \"What are some effective strategies for learning a new language?\",\n",
        "    \"How does the human immune system work to protect our body from diseases?\",\n",
        "    \"What are the key principles of good user interface design?\",\n",
        "    \"Explain the difference between artificial intelligence and machine learning.\",\n",
        "    \"What are some healthy meal prep ideas for busy professionals?\",\n",
        "    \"How can I overcome procrastination and become more productive?\",\n",
        "    \"What are the environmental benefits of renewable energy sources?\"\n",
        "]\n",
        "\n",
        "reference_answers = [\n",
        "    \"Python and JavaScript are both popular programming languages but serve different purposes. Python is primarily used for backend development, data science, and automation, with a clean, readable syntax. JavaScript is mainly used for web development, both frontend and backend (Node.js), and has more complex syntax. Python is interpreted and dynamically typed, while JavaScript runs in browsers and servers. Python has extensive libraries for scientific computing, while JavaScript excels in web interactivity and real-time applications.\",\n",
        "\n",
        "    \"To improve time management while working from home: 1) Create a dedicated workspace and stick to regular hours, 2) Use time-blocking techniques to schedule specific tasks, 3) Eliminate distractions by turning off notifications during focused work, 4) Take regular breaks using techniques like the Pomodoro method, 5) Set clear boundaries between work and personal time, 6) Prioritize tasks using methods like the Eisenhower Matrix, and 7) Use productivity tools and apps to track your time and progress.\",\n",
        "\n",
        "    \"Climate change refers to long-term shifts in global temperatures and weather patterns. Main causes include: 1) Greenhouse gas emissions from burning fossil fuels (coal, oil, gas), 2) Deforestation reducing CO2 absorption, 3) Industrial processes releasing methane and other gases, 4) Agriculture contributing to methane emissions, and 5) Transportation burning fossil fuels. These activities trap heat in Earth's atmosphere, leading to rising temperatures, melting ice caps, sea level rise, and extreme weather events.\",\n",
        "\n",
        "    \"Effective language learning strategies include: 1) Immerse yourself through movies, music, and books in the target language, 2) Practice speaking regularly with native speakers or language exchange partners, 3) Use spaced repetition systems for vocabulary building, 4) Set specific, achievable daily goals, 5) Focus on commonly used words and phrases first, 6) Practice writing in the language regularly, 7) Use language learning apps and tools consistently, and 8) Don't fear making mistakes as they're part of the learning process.\",\n",
        "\n",
        "    \"The human immune system is a complex network that protects against diseases through multiple layers: 1) Physical barriers like skin and mucous membranes, 2) Innate immunity providing immediate, general responses through white blood cells, 3) Adaptive immunity creating specific responses through T-cells and B-cells, 4) Antibodies that remember and quickly respond to previously encountered pathogens, 5) Lymph nodes filtering harmful substances, and 6) Bone marrow producing immune cells. The system coordinates to identify, attack, and remember threats.\",\n",
        "\n",
        "    \"Key principles of good UI design include: 1) Simplicity - keep interfaces clean and uncluttered, 2) Consistency - use uniform elements and patterns throughout, 3) Visibility - make important functions easily discoverable, 4) Feedback - provide clear responses to user actions, 5) Error prevention and recovery - help users avoid mistakes and fix them easily, 6) Accessibility - ensure usability for people with different abilities, 7) User control - let users feel in control of their interactions, and 8) Recognition over recall - make options visible rather than requiring memory.\",\n",
        "\n",
        "    \"Artificial Intelligence (AI) is the broader concept of machines performing tasks that typically require human intelligence, including reasoning, learning, and problem-solving. Machine Learning (ML) is a subset of AI that focuses specifically on algorithms that can learn and improve from data without being explicitly programmed for every scenario. While AI encompasses rule-based systems, expert systems, and robotics, ML specifically uses statistical techniques to enable computers to improve performance on tasks through experience. All machine learning is AI, but not all AI is machine learning.\",\n",
        "\n",
        "    \"Healthy meal prep ideas for busy professionals: 1) Batch cook proteins like grilled chicken, baked salmon, or roasted tofu, 2) Prepare grain bowls with quinoa, brown rice, and vegetables, 3) Make overnight oats with fruits and nuts for breakfast, 4) Pre-cut vegetables and store in containers for easy access, 5) Prepare soups and stews that can be frozen in portions, 6) Make energy balls or healthy snacks in advance, 7) Use sheet pan meals for easy cooking and cleanup, and 8) Invest in quality containers for proper storage and portion control.\",\n",
        "\n",
        "    \"To overcome procrastination and boost productivity: 1) Break large tasks into smaller, manageable steps, 2) Use the 'two-minute rule' - if it takes less than two minutes, do it now, 3) Eliminate decision fatigue by planning your day the night before, 4) Create accountability by sharing goals with others, 5) Remove distractions from your environment, 6) Use positive self-talk and focus on progress rather than perfection, 7) Reward yourself for completing tasks, and 8) Address underlying causes like fear of failure or perfectionism through self-reflection.\",\n",
        "\n",
        "    \"Environmental benefits of renewable energy include: 1) Significant reduction in greenhouse gas emissions compared to fossil fuels, 2) Improved air quality by eliminating pollutants that cause smog and respiratory problems, 3) Reduced water usage as most renewables require little to no water for operation, 4) Decreased environmental degradation from mining and drilling operations, 5) Protection of ecosystems and wildlife habitats, 6) Reduced acid rain from sulfur dioxide emissions, 7) Lower risk of environmental disasters like oil spills, and 8) Sustainable energy production that doesn't deplete natural resources.\"\n",
        "]\n",
        "\n",
        "\n",
        "print(\"Evaluation prompts prepared:\")\n",
        "for i, prompt in enumerate(evaluation_prompts, 1):\n",
        "    print(f\"{i}. {prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piUaVTP4uX9z"
      },
      "outputs": [],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "import re\n",
        "import evaluate\n",
        "\n",
        "bleu_metric = evaluate.load(\"bleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV5IsBuJ-iNx"
      },
      "outputs": [],
      "source": [
        "def evaluate_base_model():\n",
        "    print(\"Evaluating BASE MODEL\")\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    for i, prompt in enumerate(evaluation_prompts, 1):\n",
        "        print(f\"\\n{'-'*50}\")\n",
        "        print(f\"BASE MODEL - PROMPT {i}: {prompt}\")\n",
        "        print(f\"{'-'*50}\")\n",
        "\n",
        "        formatted_prompt = f\"\"\"<|system|>\n",
        "You are a helpful AI assistant. Provide helpful, accurate, and detailed responses to user questions.\n",
        "<|user|>\n",
        "{prompt}\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = base_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        assistant_response = response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "        print(f\"GENERATED RESPONSE:\")\n",
        "        print(assistant_response)\n",
        "\n",
        "        print(f\"\\nREFERENCE ANSWER:\")\n",
        "        print(reference_answers[i-1])\n",
        "\n",
        "        predictions.append(assistant_response)\n",
        "\n",
        "    # Clean up\n",
        "    del base_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RisuBydfucad"
      },
      "outputs": [],
      "source": [
        "def evaluate_sft_model(model_path):\n",
        "    model_name = model_path.split('/')[-1]\n",
        "    print(f\"Evaluating model: {model_name}\")\n",
        "\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for i, prompt in enumerate(evaluation_prompts, 1):\n",
        "            print(f\"\\n{'-'*50}\")\n",
        "            print(f\"{model_name.upper()} - PROMPT {i}: {prompt}\")\n",
        "            print(f\"{'-'*50}\")\n",
        "\n",
        "            formatted_prompt = f\"\"\"<|system|>\n",
        "You are a helpful AI assistant. Provide helpful, accurate, and detailed responses to user questions.\n",
        "<|user|>\n",
        "{prompt}\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=256,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            assistant_response = response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "            print(f\"GENERATED RESPONSE:\")\n",
        "            print(assistant_response)\n",
        "\n",
        "            print(f\"\\nREFERENCE ANSWER:\")\n",
        "            print(reference_answers[i-1])\n",
        "\n",
        "            predictions.append(assistant_response)\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating model {model_path}: {str(e)}\")\n",
        "        return [\"Error\"] * len(evaluation_prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nsYjpprucW9"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu_score(predictions, references):\n",
        "    references_formatted = [[ref] for ref in references]\n",
        "    bleu_result = bleu_metric.compute(predictions=predictions, references=references_formatted)\n",
        "    return bleu_result['bleu']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WUp4hrduiXW"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"EVALUATING BASE MODEL\")\n",
        "print(\"-\"*40)\n",
        "base_predictions = evaluate_base_model()\n",
        "base_bleu = calculate_bleu_score(base_predictions, reference_answers)\n",
        "evaluation_results['base_model'] = {\n",
        "    'bleu_score': base_bleu,\n",
        "    'predictions': base_predictions\n",
        "}\n",
        "print(f\"Base Model BLEU Score: {base_bleu:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOP8phw1ujui"
      },
      "outputs": [],
      "source": [
        "trial_names = list(lora_configs.keys())\n",
        "for trial_name in trial_names:\n",
        "    model_path = f\"./models/{trial_name}\"\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"\\n\" + \"-\"*40)\n",
        "        print(f\"EVALUATING {trial_name.upper()}\")\n",
        "        print(\"-\"*40)\n",
        "\n",
        "        sft_predictions = evaluate_sft_model(model_path)\n",
        "        sft_bleu = calculate_bleu_score(sft_predictions, reference_answers)\n",
        "\n",
        "        evaluation_results[trial_name] = {\n",
        "            'bleu_score': sft_bleu,\n",
        "            'predictions': sft_predictions,\n",
        "            'lora_config': lora_configs[trial_name],\n",
        "            'training_config': training_configs[trial_name]\n",
        "        }\n",
        "\n",
        "        print(f\"{trial_name} BLEU Score: {sft_bleu:.4f}\")\n",
        "        improvement = sft_bleu - base_bleu\n",
        "        print(f\"Improvement over base: {improvement:+.4f}\")\n",
        "    else:\n",
        "        print(f\"Model path {model_path} not found, skipping {trial_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tZfwG8HuFuB"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BLEU SCORE COMPARISON RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sorted_models = sorted(evaluation_results.items(), key=lambda x: x[1]['bleu_score'], reverse=True)\n",
        "\n",
        "print(f\"{'Model':<15} {'BLEU Score':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for model_name, results in sorted_models:\n",
        "    bleu_score = results['bleu_score']\n",
        "    improvement = bleu_score - evaluation_results['base_model']['bleu_score']\n",
        "    print(f\"{model_name:<15} {bleu_score:<12.4f} {improvement:+12.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtowNoM5K6qZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def save_model_responses_comparison():\n",
        "\n",
        "    print(\"Creating model responses comparison file...\")\n",
        "\n",
        "    comparison_content = \"# Model Responses Comparison for Manual Evaluation\\n\\n\"\n",
        "    comparison_content += f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
        "    comparison_content += \"---\\n\\n\"\n",
        "\n",
        "    comparison_content += \"## Summary Table\\n\\n\"\n",
        "    comparison_content += \"| Model | BLEU Score | Ranking |\\n\"\n",
        "    comparison_content += \"|-------|------------|----------|\\n\"\n",
        "\n",
        "    for rank, (model_name, results) in enumerate(sorted_models, 1):\n",
        "        comparison_content += f\"| {model_name} | {results['bleu_score']:.4f} | #{rank} |\\n\"\n",
        "\n",
        "    comparison_content += \"\\n---\\n\\n\"\n",
        "\n",
        "    for i, prompt in enumerate(evaluation_prompts):\n",
        "        comparison_content += f\"## Prompt {i+1}\\n\\n\"\n",
        "        comparison_content += f\"**Question:** {prompt}\\n\\n\"\n",
        "\n",
        "        # Reference answer\n",
        "        comparison_content += f\"### Reference Answer\\n\"\n",
        "        comparison_content += f\"{reference_answers[i]}\\n\\n\"\n",
        "\n",
        "        # Base model response\n",
        "        comparison_content += f\"### Base Model Response\\n\"\n",
        "        comparison_content += f\"{evaluation_results['base_model']['predictions'][i]}\\n\\n\"\n",
        "\n",
        "        # Fine-tuned model responses\n",
        "        for model_name, results in sorted_models:\n",
        "            if model_name != 'base_model':\n",
        "                comparison_content += f\"### {model_name} Response (BLEU: {results['bleu_score']:.4f})\\n\"\n",
        "                comparison_content += f\"{results['predictions'][i]}\\n\\n\"\n",
        "\n",
        "        comparison_content += \"---\\n\\n\"\n",
        "\n",
        "    with open(\"model_responses_comparison.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(comparison_content)\n",
        "\n",
        "    csv_data = []\n",
        "    for i, prompt in enumerate(evaluation_prompts):\n",
        "        row = {\n",
        "            'prompt_id': i + 1,\n",
        "            'prompt': prompt,\n",
        "            'reference_answer': reference_answers[i]\n",
        "        }\n",
        "\n",
        "        for model_name, results in evaluation_results.items():\n",
        "            row[f'{model_name}_response'] = results['predictions'][i]\n",
        "            row[f'{model_name}_bleu'] = results['bleu_score']\n",
        "\n",
        "        csv_data.append(row)\n",
        "\n",
        "    df_responses = pd.DataFrame(csv_data)\n",
        "    df_responses.to_csv(\"model_responses_comparison.csv\", index=False)\n",
        "\n",
        "    manual_scoring_content = \"# Manual Scoring Template\\n\\n\"\n",
        "    manual_scoring_content += \"Rate each response on a scale of 1-5 for:\\n\"\n",
        "    manual_scoring_content += \"- Accuracy (A): How factually correct is the response?\\n\"\n",
        "    manual_scoring_content += \"- Completeness (C): How well does it answer the full question?\\n\"\n",
        "    manual_scoring_content += \"- Clarity (Cl): How clear and well-structured is the response?\\n\"\n",
        "    manual_scoring_content += \"- Relevance (R): How relevant is the response to the question?\\n\\n\"\n",
        "\n",
        "    manual_scoring_content += \"Format: A=X, C=X, Cl=X, R=X, Overall=X\\n\\n\"\n",
        "    manual_scoring_content += \"---\\n\\n\"\n",
        "\n",
        "    for i, prompt in enumerate(evaluation_prompts):\n",
        "        manual_scoring_content += f\"## Prompt {i+1}: {prompt}\\n\\n\"\n",
        "\n",
        "        manual_scoring_content += \"| Model | Response | Your Scores |\\n\"\n",
        "        manual_scoring_content += \"|-------|----------|-------------|\\n\"\n",
        "\n",
        "        for model_name, results in sorted_models:\n",
        "            response = results['predictions'][i]\n",
        "            truncated_response = response[:100] + \"...\" if len(response) > 100 else response\n",
        "            truncated_response = truncated_response.replace('\\n', ' ').replace('|', '\\\\|')\n",
        "            manual_scoring_content += f\"| {model_name} | {truncated_response} | A=_, C=_, Cl=_, R=_, Overall=_ |\\n\"\n",
        "\n",
        "        manual_scoring_content += \"\\n### Full Responses:\\n\\n\"\n",
        "        for model_name, results in sorted_models:\n",
        "            manual_scoring_content += f\"**{model_name}:**\\n{results['predictions'][i]}\\n\\n\"\n",
        "\n",
        "        manual_scoring_content += \"---\\n\\n\"\n",
        "\n",
        "    with open(\"manual_scoring_template.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(manual_scoring_content)\n",
        "\n",
        "    print(\"‚úì Created model_responses_comparison.md - Full readable comparison\")\n",
        "    print(\"‚úì Created model_responses_comparison.csv - Spreadsheet format for analysis\")\n",
        "    print(\"‚úì Created manual_scoring_template.md - Template for your manual evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qglXOoEX_Hm8"
      },
      "outputs": [],
      "source": [
        "best_model_name = sorted_models[0][0]\n",
        "best_model_results = sorted_models[0][1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BEST PERFORMING MODEL\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"BLEU Score: {best_model_results['bleu_score']:.4f}\")\n",
        "\n",
        "if best_model_name != 'base_model':\n",
        "    print(f\"LoRA Configuration:\")\n",
        "    for key, value in best_model_results['lora_config'].items():\n",
        "        print(f\"  - {key}: {value}\")\n",
        "    print(f\"Training Configuration:\")\n",
        "    for key, value in best_model_results['training_config'].items():\n",
        "        print(f\"  - {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcrDa436_Q9s"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE PREDICTIONS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(min(3, len(evaluation_prompts))):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROMPT {i+1}: {evaluation_prompts[i]}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    print(f\"\\nREFERENCE ANSWER:\")\n",
        "    print(f\"{reference_answers[i]}\")\n",
        "\n",
        "    print(f\"\\nBASE MODEL PREDICTION:\")\n",
        "    print(f\"{evaluation_results['base_model']['predictions'][i]}\")\n",
        "\n",
        "    print(f\"\\nBEST MODEL ({best_model_name}) PREDICTION:\")\n",
        "    if best_model_name != 'base_model':\n",
        "        print(f\"{best_model_results['predictions'][i]}\")\n",
        "    else:\n",
        "        print(\"Same as base model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaKa0UWR_VjN"
      },
      "outputs": [],
      "source": [
        "with open(\"bleu_evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(evaluation_results, f, indent=2)\n",
        "\n",
        "best_model_info = {\n",
        "    \"best_model_name\": best_model_name,\n",
        "    \"best_model_path\": f\"./models/{best_model_name}\" if best_model_name != 'base_model' else MODEL_NAME,\n",
        "    \"bleu_score\": best_model_results['bleu_score'],\n",
        "    \"lora_config\": best_model_results.get('lora_config', None),\n",
        "    \"training_config\": best_model_results.get('training_config', None)\n",
        "}\n",
        "\n",
        "with open(\"best_sft_model_info.json\", \"w\") as f:\n",
        "    json.dump(best_model_info, f, indent=2)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best model for DPO: {best_model_name}\")\n",
        "print(f\"Best model path: {best_model_info['best_model_path']}\")\n",
        "print(f\"Files saved:\")\n",
        "print(f\"- bleu_evaluation_results.json: Detailed BLEU evaluation results\")\n",
        "print(f\"- best_sft_model_info.json: Best model information for DPO phase\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1RbDseVK6qb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(\"\\nGenerating additional analysis and visualizations...\")\n",
        "\n",
        "save_hyperparameter_analysis(training_results)\n",
        "print(\"‚úì Saved hyperparameter analysis\")\n",
        "\n",
        "save_response_analysis()\n",
        "print(\"‚úì Saved detailed response analysis\")\n",
        "\n",
        "save_model_analysis()\n",
        "print(\"‚úì Saved model analysis\")\n",
        "\n",
        "generate_summary_report()\n",
        "print(\"‚úì Generated comprehensive report\")\n",
        "\n",
        "save_comparison_plots(training_results)\n",
        "print(\"‚úì Saved comparison plots\")\n",
        "\n",
        "save_model_responses_comparison()\n",
        "print(\"‚úì Saved model responses comparison files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48MseYIiK6qb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive('output_backup', 'zip', '.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}