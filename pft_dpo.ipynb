{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 12134893,
          "sourceType": "datasetVersion",
          "datasetId": 7641971
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Preference Fine-Tuning of TinyLlama**\n",
        "\n",
        "**Assignment 05 - Part 2: PFT Implementation**"
      ],
      "metadata": {
        "id": "FGknifQBNCWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes accelerate\n",
        "!pip install -U trl\n",
        "!pip install sacrebleu\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "w4eSMX8wuywR",
        "scrolled": true,
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show trl\n"
      ],
      "metadata": {
        "id": "095b7b17-0769-4397-befa-69a2328abc3e",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "import time\n",
        "import torch\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType,\n",
        "    PeftModel\n",
        ")\n",
        "\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from evaluate import load\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "kT49tMtsqKqY",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "OunaJx7bqPlL",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "na0sL1HNYLN0",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with open(\"/kaggle/input/top-sft-model/best_sft_model_info.json\", \"r\") as f:\n",
        "        sft_info = json.load(f)\n",
        "    print(\"‚úÖ Loaded SFT model information:\")\n",
        "    print(f\"   Best SFT Model: {sft_info['best_model_name']}\")\n",
        "    print(f\"   Model Path: {sft_info['best_model_path']}\")\n",
        "    print(f\"   SFT BLEU Score: {sft_info['bleu_score']:.4f}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå SFT model info not found. Please run SFT first.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "Y2HAvv3osP_a",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_hf_cache():\n",
        "    \"\"\"Clear Hugging Face cache to avoid conflicts\"\"\"\n",
        "    cache_dirs = [\n",
        "        os.path.expanduser(\"~/.cache/huggingface/datasets\"),\n",
        "        \"/root/.cache/huggingface/datasets\",\n",
        "        \"/kaggle/working/.cache/huggingface/datasets\",\n",
        "    ]\n",
        "\n",
        "    for cache_dir in cache_dirs:\n",
        "        if os.path.exists(cache_dir):\n",
        "            try:\n",
        "                shutil.rmtree(cache_dir)\n",
        "                print(f\"‚úì Cleared cache: {cache_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† Could not clear {cache_dir}: {e}\")"
      ],
      "metadata": {
        "id": "gA_xPbEdZdXV",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dpo_mix_dataset(max_samples=None):\n",
        "    \"\"\"Load DPO Mix dataset from parquet files\"\"\"\n",
        "    print(\"üîÑ Loading DPO Mix dataset...\")\n",
        "\n",
        "    # Clear cache first\n",
        "    clear_hf_cache()\n",
        "\n",
        "    try:\n",
        "        print(\"Downloading parquet files directly...\")\n",
        "\n",
        "        # Download the train and test parquet files directly from HuggingFace\n",
        "        train_url = \"https://huggingface.co/datasets/argilla/dpo-mix-7k/resolve/main/data/train-00000-of-00001.parquet\"\n",
        "        test_url = \"https://huggingface.co/datasets/argilla/dpo-mix-7k/resolve/main/data/test-00000-of-00001.parquet\"\n",
        "\n",
        "        # Download training data\n",
        "        train_response = requests.get(train_url)\n",
        "        train_response.raise_for_status()\n",
        "        train_df = pd.read_parquet(BytesIO(train_response.content))\n",
        "\n",
        "        # Download test data\n",
        "        test_response = requests.get(test_url)\n",
        "        test_response.raise_for_status()\n",
        "        test_df = pd.read_parquet(BytesIO(test_response.content))\n",
        "\n",
        "        # Combine train and test for our purposes (we'll split later)\n",
        "        df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "        # Limit samples if specified\n",
        "        if max_samples and max_samples < len(df):\n",
        "            df = df.head(max_samples)\n",
        "            print(f\"üî™ Limited to {max_samples} samples\")\n",
        "\n",
        "        # Convert to HuggingFace dataset\n",
        "        dataset = Dataset.from_pandas(df)\n",
        "\n",
        "        dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "        print(f\"‚úÖ Successfully loaded {len(df)} samples!\")\n",
        "        return dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load dataset: {e}\")\n",
        "        raise Exception(\"Could not load dataset. Check your internet connection and try again.\")\n",
        "\n",
        "def preprocess_dpo_dataset(dataset):\n",
        "    \"\"\"Preprocess the dataset for DPO training\"\"\"\n",
        "    print(\"Preprocessing DPO dataset...\")\n",
        "\n",
        "    # Create a new dataset with the required format\n",
        "    processed_data = []\n",
        "\n",
        "    for sample in dataset['train']:\n",
        "        # Extract the prompt from the chosen messages\n",
        "        # The chosen/rejected are lists of message dicts with role/content\n",
        "        prompt = \"\"\n",
        "        chosen_response = \"\"\n",
        "        rejected_response = \"\"\n",
        "\n",
        "        # Find the user prompt (last user message before assistant response)\n",
        "        for msg in sample['chosen']:\n",
        "            if msg['role'] == 'user':\n",
        "                prompt = msg['content']\n",
        "            elif msg['role'] == 'assistant':\n",
        "                chosen_response = msg['content']\n",
        "\n",
        "        # Get rejected response (last assistant message)\n",
        "        for msg in sample['rejected']:\n",
        "            if msg['role'] == 'assistant':\n",
        "                rejected_response = msg['content']\n",
        "\n",
        "        # Skip if we couldn't extract proper prompt/response pairs\n",
        "        if not prompt or not chosen_response or not rejected_response:\n",
        "            continue\n",
        "\n",
        "        processed_sample = {\n",
        "            'prompt': prompt,\n",
        "            'chosen': chosen_response,\n",
        "            'rejected': rejected_response,\n",
        "            'chosen_rating': sample['chosen_rating'],\n",
        "            'rejected_rating': sample['rejected_rating']\n",
        "        }\n",
        "        processed_data.append(processed_sample)\n",
        "\n",
        "    # Convert to HuggingFace dataset\n",
        "    processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
        "\n",
        "    # Split into train and validation (80-20 split)\n",
        "    train_test_split = processed_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "    print(f\"Processed dataset sizes - Train: {len(train_test_split['train'])}, Val: {len(train_test_split['test'])}\")\n",
        "    return train_test_split\n",
        "\n",
        "def format_dpo_prompt(prompt):\n",
        "    \"\"\"Format the prompt for TinyLlama chat format\"\"\"\n",
        "    return f\"\"\"<|system|>\n",
        "You are a helpful AI assistant that follows instructions carefully and provides accurate responses.\n",
        "<|user|>\n",
        "{prompt}\n",
        "<|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4XkRbbHmJdcy",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dpo_mix_dataset(max_samples=3000)\n",
        "print(\"\\nüìä Dataset Information:\")\n",
        "print(f\"Dataset keys: {list(dataset.keys())}\")\n",
        "print(dataset['train'][0])\n",
        "\n",
        "dpo_dataset = preprocess_dpo_dataset(dataset)\n",
        "dpo_train_dataset = dpo_dataset['train']\n",
        "dpo_val_dataset = dpo_dataset['test']\n",
        "\n",
        "print(\"\\nSample preprocessed DPO data:\")\n",
        "print(dpo_train_dataset[0])"
      ],
      "metadata": {
        "id": "DVX0VGdAKR5u",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "def load_sft_model():\n",
        "    \"\"\"Load the best SFT model\"\"\"\n",
        "    print(\"üîÑ Loading best SFT model...\")\n",
        "\n",
        "    if sft_info['best_model_name'] == 'base_model':\n",
        "        print(\"Loading base model (no SFT applied)\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Loading SFT model: {sft_info['best_model_name']}\")\n",
        "\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "        model = PeftModel.from_pretrained(\n",
        "            base_model,\n",
        "            sft_info['best_model_path'],\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        model = model.merge_and_unload()\n",
        "\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    print(\"‚úÖ SFT model loaded successfully!\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "qroqD9iGsput",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sft_model = load_sft_model()"
      ],
      "metadata": {
        "id": "c6eWmy-dsyJE",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_lora_configs = {\n",
        "    \"dpo_trial_1\": {\n",
        "        \"r\": 4,\n",
        "        \"lora_alpha\": 16,\n",
        "        \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
        "        \"lora_dropout\": 0.05,\n",
        "    },\n",
        "    # \"dpo_trial_2\": {\n",
        "    #     \"r\": 6,\n",
        "    #     \"lora_alpha\": 24,\n",
        "    #     \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
        "    #     \"lora_dropout\": 0.05,\n",
        "    # },\n",
        "    # \"dpo_trial_3\": {\n",
        "    #     \"r\": 8,\n",
        "    #     \"lora_alpha\": 32,\n",
        "    #     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\"],\n",
        "    #     \"lora_dropout\": 0.05,\n",
        "    # },\n",
        "    # \"dpo_trial_4\": {\n",
        "    #     \"r\": 8,\n",
        "    #     \"lora_alpha\": 32,\n",
        "    #     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    #     \"lora_dropout\": 0.05,\n",
        "    # },\n",
        "    \"dpo_trial_5\": {\n",
        "        \"r\": 12,\n",
        "        \"lora_alpha\": 48,\n",
        "        \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "        \"lora_dropout\": 0.05,\n",
        "    },\n",
        "}\n",
        "\n",
        "dpo_training_configs = {\n",
        "    \"dpo_trial_1\": {\n",
        "        \"learning_rate\": 5e-6,\n",
        "        \"per_device_train_batch_size\": 4,\n",
        "        \"num_train_epochs\": 2,\n",
        "        \"gradient_accumulation_steps\": 3,\n",
        "        \"beta\": 0.1,\n",
        "    },\n",
        "    # \"dpo_trial_2\": {\n",
        "    #     \"learning_rate\": 6e-6,\n",
        "    #     \"per_device_train_batch_size\": 3,\n",
        "    #     \"num_train_epochs\": 2,\n",
        "    #     \"gradient_accumulation_steps\": 2,\n",
        "    #     \"beta\": 0.12,\n",
        "    # },\n",
        "    # \"dpo_trial_3\": {\n",
        "    #     \"learning_rate\": 7e-6,\n",
        "    #     \"per_device_train_batch_size\": 2,\n",
        "    #     \"num_train_epochs\": 3,\n",
        "    #     \"gradient_accumulation_steps\": 4,\n",
        "    #     \"beta\": 0.15,\n",
        "    # },\n",
        "    # \"dpo_trial_4\": {\n",
        "    #     \"learning_rate\": 1e-5,\n",
        "    #     \"per_device_train_batch_size\": 4,\n",
        "    #     \"num_train_epochs\": 3,\n",
        "    #     \"gradient_accumulation_steps\": 5,\n",
        "    #     \"beta\": 0.2,\n",
        "    # },\n",
        "    \"dpo_trial_5\": {\n",
        "        \"learning_rate\": 1.2e-5,\n",
        "        \"per_device_train_batch_size\": 2,\n",
        "        \"num_train_epochs\": 3,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"beta\": 0.25,\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "weWVIonGOsHt",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dpo_model(trial_name, lora_config, training_config, sft_model):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training DPO {trial_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    model_copy = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    if sft_info['best_model_name'] != 'base_model':\n",
        "        model_copy = PeftModel.from_pretrained(\n",
        "            model_copy,\n",
        "            sft_info['best_model_path'],\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        model_copy = model_copy.merge_and_unload()\n",
        "\n",
        "    model_copy = prepare_model_for_kbit_training(model_copy)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=lora_config[\"r\"],\n",
        "        lora_alpha=lora_config[\"lora_alpha\"],\n",
        "        target_modules=lora_config[\"target_modules\"],\n",
        "        lora_dropout=lora_config[\"lora_dropout\"],\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    model_copy = get_peft_model(model_copy, peft_config)\n",
        "    model_copy.print_trainable_parameters()\n",
        "\n",
        "    ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    if sft_info['best_model_name'] != 'base_model':\n",
        "        ref_model = PeftModel.from_pretrained(\n",
        "            ref_model,\n",
        "            sft_info['best_model_path'],\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        ref_model = ref_model.merge_and_unload()\n",
        "\n",
        "    training_args = DPOConfig(\n",
        "        output_dir=f\"./dpo_results/{trial_name}\",\n",
        "        learning_rate=training_config[\"learning_rate\"],\n",
        "        per_device_train_batch_size=training_config[\"per_device_train_batch_size\"],\n",
        "        per_device_eval_batch_size=1,\n",
        "        num_train_epochs=training_config[\"num_train_epochs\"],\n",
        "        gradient_accumulation_steps=training_config[\"gradient_accumulation_steps\"],\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        logging_steps=120,\n",
        "        save_strategy=\"epoch\",\n",
        "        fp16=True,\n",
        "        remove_unused_columns=False,\n",
        "        run_name=f\"dpo_{trial_name}\",\n",
        "        dataloader_drop_last=True,\n",
        "        beta=training_config[\"beta\"],\n",
        "        max_length=512,\n",
        "        max_prompt_length=256,\n",
        "    )\n",
        "\n",
        "    def format_dataset_for_dpo(examples):\n",
        "        \"\"\"Format the dataset for general instruction-following\"\"\"\n",
        "        formatted_examples = {\n",
        "            'prompt': [],\n",
        "            'chosen': [],\n",
        "            'rejected': []\n",
        "        }\n",
        "\n",
        "        for i in range(len(examples['prompt'])):\n",
        "            formatted_prompt = f\"\"\"<|system|>\n",
        "You are a helpful AI assistant that follows instructions carefully and provides accurate, concise, and informative responses.\n",
        "<|user|>\n",
        "{examples['prompt'][i]}\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "            formatted_examples['prompt'].append(formatted_prompt)\n",
        "            formatted_examples['chosen'].append(examples['chosen'][i])\n",
        "            formatted_examples['rejected'].append(examples['rejected'][i])\n",
        "\n",
        "        return formatted_examples\n",
        "\n",
        "    formatted_train_dataset = dpo_train_dataset.map(\n",
        "        format_dataset_for_dpo,\n",
        "        batched=True,\n",
        "        remove_columns=dpo_train_dataset.column_names\n",
        "    )\n",
        "\n",
        "    formatted_val_dataset = dpo_val_dataset.map(\n",
        "        format_dataset_for_dpo,\n",
        "        batched=True,\n",
        "        remove_columns=dpo_val_dataset.column_names\n",
        "    )\n",
        "\n",
        "    dpo_trainer = DPOTrainer(\n",
        "        model=model_copy,\n",
        "        ref_model=None,\n",
        "        args=training_args,\n",
        "        train_dataset=formatted_train_dataset,\n",
        "        eval_dataset=formatted_val_dataset,\n",
        "        processing_class=tokenizer,\n",
        "        peft_config=peft_config,\n",
        "    )\n",
        "\n",
        "    print(\"Starting DPO training...\")\n",
        "    dpo_trainer.train()\n",
        "    dpo_trainer.save_model(f\"./dpo_models/{trial_name}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    print(f\"{trial_name} completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    training_logs = {\n",
        "        \"train_loss_history\": dpo_trainer.state.log_history,\n",
        "        \"eval_loss_history\": [log for log in dpo_trainer.state.log_history if 'eval_loss' in log],\n",
        "        \"learning_rate_schedule\": [log.get('learning_rate', 0) for log in dpo_trainer.state.log_history],\n",
        "        \"global_steps\": dpo_trainer.state.global_step,\n",
        "        \"total_epochs\": dpo_trainer.state.epoch,\n",
        "    }\n",
        "\n",
        "    with open(f\"dpo_training_metrics_{trial_name}.json\", \"w\") as f:\n",
        "        json.dump(training_logs, f, indent=2)\n",
        "\n",
        "    del model_copy\n",
        "    del ref_model\n",
        "    del dpo_trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        \"trial_name\": trial_name,\n",
        "        \"training_time\": training_time,\n",
        "        \"lora_config\": lora_config,\n",
        "        \"training_config\": training_config\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_dpo_dataset(dataset):\n",
        "    \"\"\"Extract prompt, chosen, and rejected from multi-turn chat structure\"\"\"\n",
        "    print(\"üîÑ Preprocessing DPO dataset...\")\n",
        "\n",
        "    processed_data = []\n",
        "\n",
        "    for sample in dataset['train']:\n",
        "        prompt = \"\"\n",
        "        chosen_response = \"\"\n",
        "        rejected_response = \"\"\n",
        "\n",
        "        for msg in sample['chosen']:\n",
        "            if msg['role'] == 'user':\n",
        "                prompt = msg['content']\n",
        "            elif msg['role'] == 'assistant':\n",
        "                chosen_response = msg['content']\n",
        "\n",
        "        for msg in sample['rejected']:\n",
        "            if msg['role'] == 'assistant':\n",
        "                rejected_response = msg['content']\n",
        "\n",
        "        if not prompt or not chosen_response or not rejected_response:\n",
        "            continue\n",
        "\n",
        "        processed_data.append({\n",
        "            'prompt': prompt,\n",
        "            'chosen': chosen_response,\n",
        "            'rejected': rejected_response,\n",
        "            'chosen_rating': sample['chosen_rating'],\n",
        "            'rejected_rating': sample['rejected_rating']\n",
        "        })\n",
        "\n",
        "    if not processed_data:\n",
        "        raise ValueError(\"‚ùå No valid samples found in dataset.\")\n",
        "\n",
        "    # Convert to HuggingFace Dataset and split\n",
        "    processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
        "    split_dataset = processed_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "    print(f\"‚úÖ Processed dataset sizes - Train: {len(split_dataset['train'])}, Val: {len(split_dataset['test'])}\")\n",
        "    return split_dataset\n"
      ],
      "metadata": {
        "id": "7YCWt4Nko8vw",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING DPO TRAINING EXPERIMENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dpo_results = []\n",
        "\n",
        "for trial_name in dpo_lora_configs.keys():\n",
        "    try:\n",
        "        result = train_dpo_model(\n",
        "            trial_name=trial_name,\n",
        "            lora_config=dpo_lora_configs[trial_name],\n",
        "            training_config=dpo_training_configs[trial_name],\n",
        "            sft_model=sft_model\n",
        "        )\n",
        "\n",
        "        dpo_results.append(result)\n",
        "\n",
        "        with open(f\"dpo_training_results_{trial_name}.json\", \"w\") as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in {trial_name}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue"
      ],
      "metadata": {
        "id": "33D8zQAFt_6h",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL TRIALS COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with open(\"all_dpo_training_results.json\", \"w\") as f:\n",
        "    json.dump(dpo_results, f, indent=2)"
      ],
      "metadata": {
        "id": "F-LNxz78uD57",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_prompts = [\n",
        "    \"Explain the difference between machine learning and deep learning.\",\n",
        "    \"Write a short paragraph describing the benefits of regular exercise.\",\n",
        "    \"What is the capital of France, and why is it famous?\",\n",
        "    \"Summarize the plot of Romeo and Juliet.\",\n",
        "    \"How do I bake a chocolate cake from scratch?\",\n",
        "    \"Give three reasons why climate change is a global concern.\",\n",
        "    \"Convert the sentence 'He is reading a book' into passive voice.\",\n",
        "    \"What are the pros and cons of remote work?\",\n",
        "    \"Write a polite email requesting a deadline extension.\",\n",
        "    \"Translate the phrase 'Good morning' into Spanish, French, and German.\"\n",
        "]\n",
        "\n",
        "reference_answers = [\n",
        "    \"Machine learning is a subset of AI that focuses on building systems that learn from data. Deep learning is a type of machine learning that uses neural networks with many layers to analyze complex patterns.\",\n",
        "    \"Regular exercise improves cardiovascular health, strengthens muscles, boosts mood, and helps maintain a healthy weight.\",\n",
        "    \"The capital of France is Paris. It is famous for landmarks like the Eiffel Tower, its art and culture, fashion industry, and rich history.\",\n",
        "    \"Romeo and Juliet is a tragedy by William Shakespeare about two young lovers from feuding families whose deaths ultimately reconcile their families.\",\n",
        "    \"To bake a chocolate cake from scratch, mix flour, sugar, cocoa powder, eggs, butter, and baking soda. Pour the batter into a greased pan and bake at 350¬∞F for 30‚Äì35 minutes.\",\n",
        "    \"Climate change is a global concern because it leads to rising sea levels, more frequent extreme weather events, and disruptions to ecosystems and agriculture.\",\n",
        "    \"The passive voice of 'He is reading a book' is 'A book is being read by him.'\",\n",
        "    \"Remote work offers flexibility and saves commuting time, but it can also lead to isolation and distractions at home.\",\n",
        "    \"Dear Professor, I hope you're doing well. I'm writing to request an extension for the assignment deadline due to unforeseen circumstances. Thank you for your consideration.\",\n",
        "    \"Spanish: Buenos d√≠as, French: Bonjour, German: Guten Morgen.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "KqBA3jqCuPY9",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "import re\n",
        "import evaluate\n",
        "\n",
        "bleu_metric = evaluate.load(\"bleu\")"
      ],
      "metadata": {
        "id": "piUaVTP4uX9z",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_dpo_model(model_path, trial_name):\n",
        "    print(f\"Evaluating DPO model: {trial_name}\")\n",
        "\n",
        "    try:\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "        if sft_info['best_model_name'] != 'base_model':\n",
        "            base_model = PeftModel.from_pretrained(\n",
        "                base_model,\n",
        "                sft_info['best_model_path'],\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            base_model = base_model.merge_and_unload()\n",
        "\n",
        "        model = PeftModel.from_pretrained(\n",
        "            base_model,\n",
        "            model_path,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for i, prompt in enumerate(evaluation_prompts, 1):\n",
        "            print(f\"\\n{'-'*50}\")\n",
        "            print(f\"{trial_name.upper()} - PROMPT {i}: {prompt}\")\n",
        "            print(f\"{'-'*50}\")\n",
        "\n",
        "            formatted_prompt = f\"\"\"<|system|>\n",
        "You are a helpful AI assistant that follows instructions carefully and provides accurate, concise, and informative responses.\n",
        "<|user|>\n",
        "{prompt}\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=256,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            assistant_response = response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "            print(f\"GENERATED RESPONSE:\")\n",
        "            print(assistant_response)\n",
        "\n",
        "            print(f\"\\nREFERENCE ANSWER:\")\n",
        "            print(reference_answers[i-1])\n",
        "\n",
        "            predictions.append(assistant_response)\n",
        "\n",
        "        del model\n",
        "        del base_model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating DPO model {model_path}: {str(e)}\")\n",
        "        return [\"Error\"] * len(evaluation_prompts)"
      ],
      "metadata": {
        "id": "YR-9tAXdtspt",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_score(predictions, references):\n",
        "    \"\"\"Calculate BLEU score between predictions and references\"\"\"\n",
        "    bleu_metric = load(\"bleu\")\n",
        "    references_formatted = [[ref] for ref in references]\n",
        "    bleu_result = bleu_metric.compute(predictions=predictions, references=references_formatted)\n",
        "    return bleu_result['bleu']\n"
      ],
      "metadata": {
        "id": "tjtbO_0ZNorf",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING DPO MODEL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dpo_evaluation_results = {}"
      ],
      "metadata": {
        "id": "QB2jJ96mt_0u",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for trial_name in dpo_lora_configs.keys():\n",
        "    model_path = f\"./dpo_models/{trial_name}\"\n",
        "    if os.path.exists(model_path):\n",
        "        print(\"\\n\" + \"-\" * 40)\n",
        "        print(f\"EVALUATING DPO {trial_name.upper()}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        dpo_predictions = evaluate_dpo_model(model_path, trial_name)\n",
        "        dpo_bleu = calculate_bleu_score(dpo_predictions, reference_answers)\n",
        "\n",
        "        dpo_evaluation_results[trial_name] = {\n",
        "            'bleu_score': dpo_bleu,\n",
        "            'predictions': dpo_predictions,\n",
        "            'lora_config': dpo_lora_configs[trial_name],\n",
        "            'training_config': dpo_training_configs[trial_name]\n",
        "        }\n",
        "\n",
        "        print(f\"{trial_name} BLEU Score: {dpo_bleu:.4f}\")\n",
        "\n",
        "        if sft_info['best_model_name'] in sft_evaluation_results:\n",
        "            sft_bleu = sft_evaluation_results[sft_info['best_model_name']]['bleu_score']\n",
        "            improvement = dpo_bleu - sft_bleu\n",
        "            print(f\"Improvement over best SFT: {improvement:+.4f}\")\n",
        "    else:\n",
        "        print(f\"DPO model path {model_path} not found, skipping {trial_name}\")\n"
      ],
      "metadata": {
        "id": "p98-l_VWuGRU",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DPO EVALUATION RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sorted_dpo_models = sorted(dpo_evaluation_results.items(), key=lambda x: x[1]['bleu_score'], reverse=True)\n",
        "\n",
        "print(f\"{'BLEU Score':<12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for model_name, results in sorted_dpo_models:\n",
        "    bleu_score = results['bleu_score']\n",
        "    print(f\"{model_name:<15} {bleu_score:<12.4f}\")\n"
      ],
      "metadata": {
        "id": "dmdkYAqWuG-j",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if sorted_dpo_models:\n",
        "    best_dpo_name = sorted_dpo_models[0][0]\n",
        "    best_dpo_results = sorted_dpo_models[0][1]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BEST DPO MODEL\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Best DPO Model: {best_dpo_name}\")\n",
        "    print(f\"BLEU Score: {best_dpo_results['bleu_score']:.4f}\")\n",
        "\n",
        "    print(f\"\\nLoRA Configuration:\")\n",
        "    for key, value in best_dpo_results['lora_config'].items():\n",
        "        print(f\"  - {key}: {value}\")\n",
        "\n",
        "    print(f\"\\nDPO Training Configuration:\")\n",
        "    for key, value in best_dpo_results['training_config'].items():\n",
        "        print(f\"  - {key}: {value}\")\n"
      ],
      "metadata": {
        "id": "sVqgVUEPuLcz",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_detailed_analysis():\n",
        "    \"\"\"Generate detailed analysis and insights (failsafe)\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"DETAILED ANALYSIS AND INSIGHTS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    insights = {}\n",
        "\n",
        "    if not dpo_evaluation_results:\n",
        "        print(\"No DPO evaluation results found. Skipping analysis.\")\n",
        "        return insights\n",
        "\n",
        "    try:\n",
        "        sorted_dpo_models = sorted(\n",
        "            dpo_evaluation_results.items(),\n",
        "            key=lambda x: x[1].get('bleu_score', -1),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        if sorted_dpo_models:\n",
        "            best_dpo = sorted_dpo_models[0]\n",
        "            worst_dpo = sorted_dpo_models[-1]\n",
        "\n",
        "            insights['performance'] = {\n",
        "                'best_dpo_model': best_dpo[0],\n",
        "                'best_dpo_bleu': best_dpo[1]['bleu_score'],\n",
        "                'worst_dpo_model': worst_dpo[0],\n",
        "                'worst_dpo_bleu': worst_dpo[1]['bleu_score'],\n",
        "                'performance_gap': best_dpo[1]['bleu_score'] - worst_dpo[1]['bleu_score']\n",
        "            }\n",
        "\n",
        "            print(f\"Best DPO Model: {best_dpo[0]} (BLEU: {best_dpo[1]['bleu_score']:.4f})\")\n",
        "            print(f\"Worst DPO Model: {worst_dpo[0]} (BLEU: {worst_dpo[1]['bleu_score']:.4f})\")\n",
        "            print(f\"Performance Gap: {insights['performance']['performance_gap']:.4f}\")\n",
        "        else:\n",
        "            print(\"No sorted DPO models found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to evaluate best/worst DPO models: {e}\")\n",
        "\n",
        "    print(f\"\\nCONFIGURATION IMPACT ANALYSIS:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    rank_performance = {}\n",
        "    for name, results in dpo_evaluation_results.items():\n",
        "        rank = results.get('lora_config', {}).get('r')\n",
        "        if rank is not None:\n",
        "            rank_performance.setdefault(rank, []).append(results.get('bleu_score', 0))\n",
        "\n",
        "    print(\"LoRA Rank Impact:\")\n",
        "    if rank_performance:\n",
        "        for rank, scores in rank_performance.items():\n",
        "            avg_score = np.mean(scores)\n",
        "            print(f\"  Rank {rank}: Average BLEU = {avg_score:.4f} (n={len(scores)})\")\n",
        "    else:\n",
        "        print(\"  No rank data available.\")\n",
        "\n",
        "    # Beta parameter impact\n",
        "    beta_performance = {}\n",
        "    for name, results in dpo_evaluation_results.items():\n",
        "        beta = results.get('training_config', {}).get('beta')\n",
        "        if beta is not None:\n",
        "            beta_performance[beta] = results.get('bleu_score', 0)\n",
        "\n",
        "    print(\"\\nDPO Beta Parameter Impact:\")\n",
        "    if beta_performance:\n",
        "        for beta, score in sorted(beta_performance.items()):\n",
        "            print(f\"  Beta {beta}: BLEU = {score:.4f}\")\n",
        "    else:\n",
        "        print(\"  No beta data available.\")\n",
        "\n",
        "    lr_performance = {}\n",
        "    for name, results in dpo_evaluation_results.items():\n",
        "        lr = results.get('training_config', {}).get('learning_rate')\n",
        "        if lr is not None:\n",
        "            lr_performance[lr] = results.get('bleu_score', 0)\n",
        "\n",
        "    print(\"\\nLearning Rate Impact:\")\n",
        "    if lr_performance:\n",
        "        for lr, score in sorted(lr_performance.items()):\n",
        "            print(f\"  LR {lr}: BLEU = {score:.4f}\")\n",
        "    else:\n",
        "        print(\"  No learning rate data available.\")\n",
        "\n",
        "    insights['configuration_impact'] = {\n",
        "        'rank_performance': rank_performance,\n",
        "        'beta_performance': beta_performance,\n",
        "        'lr_performance': lr_performance\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìà IMPROVEMENT OVER BEST SFT:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    try:\n",
        "        sft_baseline_name = sft_info.get('best_model_name')\n",
        "        sft_baseline = sft_evaluation_results[sft_baseline_name]['bleu_score']\n",
        "\n",
        "        print(f\"Best SFT BLEU Score: {sft_baseline:.4f}\")\n",
        "\n",
        "        improvements = {}\n",
        "        for name, results in dpo_evaluation_results.items():\n",
        "            dpo_bleu = results.get('bleu_score')\n",
        "            if dpo_bleu is not None:\n",
        "                improvement = dpo_bleu - sft_baseline\n",
        "                improvements[name] = improvement\n",
        "                status = \"‚úÖ Improved\" if improvement > 0 else \"‚ùå Degraded\"\n",
        "                print(f\"{name}: {improvement:+.4f} {status}\")\n",
        "\n",
        "        insights['sft_comparison'] = {\n",
        "            'sft_baseline': sft_baseline,\n",
        "            'improvements': improvements,\n",
        "            'models_improved': sum(1 for imp in improvements.values() if imp > 0),\n",
        "            'models_degraded': sum(1 for imp in improvements.values() if imp < 0)\n",
        "        }\n",
        "\n",
        "        total = len(improvements)\n",
        "        print(f\"\\nSummary: {insights['sft_comparison']['models_improved']}/{total} models improved over SFT\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to compute SFT comparison: {e}\")\n",
        "        insights['sft_comparison'] = {}\n",
        "\n",
        "    return insight"
      ],
      "metadata": {
        "id": "dvudTwlhuO3p",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_insights = generate_detailed_analysis()"
      ],
      "metadata": {
        "id": "wu78yNlqMjou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_comprehensive_results():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAVING COMPREHENSIVE RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    final_results = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'sft_info': sft_info,\n",
        "        'dpo_evaluation_results': dpo_evaluation_results,\n",
        "        'analysis_insights': analysis_insights,\n",
        "        'experiment_summary': {\n",
        "            'total_dpo_models_trained': len(dpo_lora_configs),\n",
        "            'total_dpo_models_evaluated': len(dpo_evaluation_results),\n",
        "            'best_dpo_model': sorted_dpo_models[0][0] if sorted_dpo_models else None,\n",
        "            'best_dpo_bleu': sorted_dpo_models[0][1]['bleu_score'] if sorted_dpo_models else None,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(\"final_dpo_results.json\", \"w\") as f:\n",
        "        json.dump(final_results, f, indent=2, default=str)\n",
        "\n",
        "    recommendations = {\n",
        "        'best_overall_model': sorted_dpo_models[0][0] if sorted_dpo_models else None,\n",
        "        'recommended_configs': {},\n",
        "        'deployment_notes': []\n",
        "    }\n",
        "\n",
        "    if sorted_dpo_models:\n",
        "        best_model = sorted_dpo_models[0]\n",
        "        recommendations['recommended_configs'] = {\n",
        "            'lora_config': best_model[1]['lora_config'],\n",
        "            'training_config': best_model[1]['training_config']\n",
        "        }\n",
        "\n",
        "        recommendations['deployment_notes'] = [\n",
        "            f\"Use {best_model[0]} for production deployment\",\n",
        "            f\"Expected BLEU score: {best_model[1]['bleu_score']:.4f}\",\n",
        "            \"Model shows good balance across all evaluation criteria\"\n",
        "        ]\n",
        "\n",
        "    with open(\"model_recommendations.json\", \"w\") as f:\n",
        "        json.dump(recommendations, f, indent=2)\n",
        "\n",
        "    summary_report = f\"\"\"\n",
        "DPO FINE-TUNING EXPERIMENT SUMMARY REPORT\n",
        "========================================\n",
        "\n",
        "Experiment Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "DATASET INFORMATION:\n",
        "- DPO Dataset: Math-Step-DPO-10K\n",
        "- Training Samples: {len(dpo_train_dataset)}\n",
        "- Validation Samples: {len(dpo_val_dataset)}\n",
        "\n",
        "MODEL TRAINING:\n",
        "- Base Model: {MODEL_NAME}\n",
        "- SFT Foundation: {sft_info['best_model_name']}\n",
        "- DPO Models Trained: {len(dpo_lora_configs)}\n",
        "- DPO Models Evaluated: {len(dpo_evaluation_results)}\n",
        "\n",
        "PERFORMANCE RESULTS:\n",
        "\"\"\"\n",
        "\n",
        "    if sorted_dpo_models:\n",
        "        summary_report += f\"\"\"\n",
        "BEST PERFORMING MODEL: {sorted_dpo_models[0][0]}\n",
        "- BLEU Score: {sorted_dpo_models[0][1]['bleu_score']:.4f}\n",
        "\n",
        "CONFIGURATION:\n",
        "- LoRA Rank: {sorted_dpo_models[0][1]['lora_config']['r']}\n",
        "- LoRA Alpha: {sorted_dpo_models[0][1]['lora_config']['lora_alpha']}\n",
        "- Target Modules: {sorted_dpo_models[0][1]['lora_config']['target_modules']}\n",
        "- Learning Rate: {sorted_dpo_models[0][1]['training_config']['learning_rate']}\n",
        "- DPO Beta: {sorted_dpo_models[0][1]['training_config']['beta']}\n",
        "\"\"\"\n",
        "\n",
        "    summary_report += f\"\"\"\n",
        "EVALUATION PROMPTS: {len(evaluation_prompts)} math word problems\n",
        "# EVALUATION METRICS: BLEU score + Manual criteria (Helpfulness, Relevance, Accuracy, Harmlessness)\n",
        "\n",
        "FILES GENERATED:\n",
        "- final_dpo_results.json: Comprehensive results\n",
        "- model_recommendations.json: Deployment recommendations\n",
        "- dpo_evaluation_results.png: Performance visualizations\n",
        "- comprehensive_model_comparison.png: Model comparison chart\n",
        "\"\"\"\n",
        "\n",
        "    with open(\"dpo_experiment_summary.txt\", \"w\") as f:\n",
        "        f.write(summary_report)\n",
        "\n",
        "    print(\"All results saved successfully!\")\n",
        "    print(\"Generated files:\")\n",
        "    print(\"   - final_dpo_results.json\")\n",
        "    print(\"   - model_recommendations.json\")\n",
        "    print(\"   - dpo_experiment_summary.txt\")\n",
        "    print(\"   - dpo_evaluation_results.png\")\n",
        "    print(\"   - comprehensive_model_comparison.png\")"
      ],
      "metadata": {
        "id": "hcWyTk-Dz4x4",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "save_comprehensive_results()"
      ],
      "metadata": {
        "id": "hvQDYUIfM2xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive('output_backup', 'zip', '.')"
      ],
      "metadata": {
        "trusted": true,
        "id": "pkw4GjZgK7_b"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}